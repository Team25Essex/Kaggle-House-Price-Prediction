import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model  import LinearRegression
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
import lightgbm as lgb
from lightgbm import LGBMRegressor 
import category_encoders as ce
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.multiclass import OneVsRestClassifier
import numpy as np

def Features(x, y):
       
        logistic = LogisticRegression(C=0.01, penalty="l1",solver='liblinear', random_state=100).fit(x, y)
        model = SelectFromModel(logistic, prefit=True)
        x_new = model.transform(x)

        # Get back the kept features as a DataFrame with dropped columns as all 0s
        selected_features = pd.DataFrame(model.inverse_transform(x_new), 
                                        index=x.index,
                                        columns=x.columns)
        
        # Dropped columns have values of all 0s, keep other columns 
        good_columns = selected_features.columns[selected_features.var() != 0]
        
        return good_columns
    
    




test = pd.read_csv("../input/house-prices-advanced-regression-techniques/test.csv")
test2 = test.copy()
train = pd.read_csv("../input/house-prices-advanced-regression-techniques/train.csv")
train2=train.copy()
train.dropna(axis=0,subset=['SalePrice'],inplace=True)
Train_y = train.SalePrice   #The target columns without NaN
train.drop(['Id','Street','Utilities','LowQualFinSF','3SsnPorch','PoolArea','PoolQC'],axis=1,inplace=True)
test.drop(['Id','Street','Utilities','LowQualFinSF','3SsnPorch','PoolArea','PoolQC'],axis=1,inplace=True)
train.drop(['SalePrice'],axis=1,inplace=True)  #The predictive columns





#for category part:
drop_X = train.select_dtypes(include=['object'])
drop_test_X = test.select_dtypes(include=['object'])

#fill the mising value using SimpleImputer:
SI= SimpleImputer(strategy='constant',fill_value='None')
SI_X = pd.DataFrame(SI.fit_transform(drop_X))
SI_Test_X= pd.DataFrame(SI.transform(drop_test_X))
SI_X.columns = drop_X.columns
SI_Test_X.columns = drop_test_X.columns

#encode the category data:
features= SI_X.columns
CE = ce.CatBoostEncoder(cols=SI_X.columns)
train_copy=pd.concat([train2.select_dtypes(include=['object']),train2['SalePrice']],axis=1)
CE.fit(train_copy[features], train_copy['SalePrice'])

a = CE.transform(SI_X[features])
b = CE.transform(SI_Test_X[features])


#for numerical part:
number_X = train.select_dtypes(exclude=['object'])
number_test_X = test.select_dtypes(exclude=['object'])
#fill the mising value using SimpleImputer:
SI_Median = SimpleImputer(strategy='constant',fill_value=0)
SI_Median_X = pd.DataFrame(SI_Median.fit_transform(number_X))
SI_Median_test_X = pd.DataFrame(SI_Median.transform(number_test_X))
SI_Median_X.columns = number_X.columns
SI_Median_test_X.columns = number_test_X.columns



#combine numerical part and categorical part:
union_x=pd.concat([a,SI_Median_X],axis=1)
union_test_x=pd.concat([b,SI_Median_test_X],axis=1)
light_train_x=pd.concat([union_x,train2['SalePrice']],axis=1)





#Split the datasize
valid_fraction = 0.2741
valid_size = int(len(union_x) * valid_fraction)
train_train = light_train_x[:-2 * valid_size]
train_valid = light_train_x[-2 * valid_size:]
feature_cols = train_train.columns.drop('SalePrice')
train_train_x = train_train[feature_cols]
train_valid_x = train_valid[feature_cols]
train_train_y = train_train['SalePrice']
train_valid_y = train_valid['SalePrice']
for each in [train_train, train_valid]:
    print(each['SalePrice'].mean())

    
#Regularization and remain the good columns:
Object_Columns=Features(train_train_x,train_train_y)
Remained_train_X = train_train_x[Object_Columns]
Remained_valid_X = train_valid_x[Object_Columns]
Remained_test_X = union_test_x[Object_Columns] 


#Light

prepare = LGBMRegressor(metric='mae',num_leaves= 60,num_round = 200000,objective='regression', learning_rate=0.01, n_estimators=2000)
prepare.fit(Remained_train_X,train_train_y,eval_set=[(Remained_valid_X,train_valid_y)], early_stopping_rounds=2000)
bestprediction4 = prepare.predict(Remained_test_X)

#XGBOOST
my_model = XGBRegressor(n_estimators=200000,objective ='reg:squarederror', learning_rate=0.01)
my_model.fit(Remained_train_X, train_train_y, 
             early_stopping_rounds=2000, 
             eval_set=[(Remained_valid_X, train_valid_y)], 
             verbose=False)

#Using Linear Regression model:
bestm1=LinearRegression()
bestm1.fit(Remained_train_X,train_train_y)
bestprediction1=bestm1.predict(Remained_test_X)


#RandomTreeRegressor
bestm2 = RandomForestRegressor(random_state=5)
bestm2.fit(Remained_train_X,train_train_y)
bestprediction2=bestm2.predict(Remained_test_X)


bestprediction3=my_model.predict(Remained_test_X)
bestprediction=bestprediction4

predictionframe = pd.DataFrame({'Id':test2.Id,
                       'SalePrice': bestprediction})
predictionframe.to_csv('submission.csv', index=False)
